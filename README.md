# Chat_with_Local_Documents
This project is a Streamlit-based application that allows users to chat with their local documents using Retrieval-Augmented Generation (RAG). The app integrates LangChain, ChromaDB, and Ollama (with a local deepseek-r1:8b model) to retrieve relevant document chunks and generate contextual answers.
## ğŸš€ Features

Document ingestion: Automatically loads and chunks text files from the current working directory.

Vector database: Stores and retrieves embeddings using ChromaDB and SentenceTransformer embeddings.

RAG pipeline: Retrieves the most relevant document chunks for each query.

Local LLM: Uses Ollama with the deepseek-r1:8b model for answering questions.

Chat interface: Built with Streamlitâ€™s chat UI for interactive Q&A.

Transparent reasoning: Separates the modelâ€™s <think> process (reasoning) from the final response, shown in an expandable section.

## ğŸ› ï¸ Tech Stack

LangChain â€“ framework for LLM-powered applications

Chroma â€“ vector database for storing embeddings

SentenceTransformers â€“ for generating embeddings (all-MiniLM-L6-v2)

Ollama â€“ local LLM runner

Streamlit â€“ frontend interface

Python standard libraries: datetime, json, os, time
## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ app.py                # Main Streamlit application (your code)
â”œâ”€â”€ requirements.txt      # Dependencies (see below)
â”œâ”€â”€ documents/            # Directory where your local documents should be stored
â””â”€â”€ README.md             # Project documentation
```
## ğŸ“¦ Installation

Clone the repository:
```
git clone https://github.com/yourusername/chat-with-local-docs.git
cd chat-with-local-docs
```
Create a virtual environment and install dependencies:
```
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```
Install Ollama and pull the required model:
```
ollama pull deepseek-r1:8b
```
## â–¶ï¸ Usage

Run the Streamlit app:
```
streamlit run app.py
```
Then, open the provided local URL (e.g., http://localhost:8501) in your browser.
## ğŸ’¡ How it Works

Load Documents: The app scans the current directory for text-based documents.

Split & Embed: Documents are split into 1000-character chunks and embedded using all-MiniLM-L6-v2.

ChromaDB: Embeddings are stored in a local vector store.

RAG Query: When a user asks a question, the top 5 most similar document chunks are retrieved.

LLM Answer: The context and question are passed to Ollama (deepseek-r1:8b), which generates a response.

Chat UI: Streamlit displays the modelâ€™s thought process (if available) and final answer.

## âš ï¸ Notes

Make sure Ollama is installed and running in the background.

Replace deepseek-r1:8b with any other locally available model in get_local_model().

The reasoning process (<think> ... </think>) may not always appear depending on the model used.
## ğŸ“Œ Roadmap

 Support for PDF, DOCX, and CSV loaders

 Option to upload documents via UI

 Persistent vector store (instead of in-memory)

 Multi-model support (choose from different Ollama models)
